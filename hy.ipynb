{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac924cd",
   "metadata": {},
   "source": [
    "## Cell 1: Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306a00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da359966",
   "metadata": {},
   "source": [
    "## Cell 2: Load & Merge with Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333188de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\\Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\\Monday-WorkingHours.pcap_ISCX.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\\Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\\Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\\Wednesday-workingHours.pcap_ISCX.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\02-14-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\02-15-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\02-16-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\02-20-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\02-21-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\02-22-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\02-23-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\02-28-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\03-01-2018.csv\n",
      "Loading: C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\\03-02-2018.csv\n"
     ]
    }
   ],
   "source": [
    "def load_merge_files(folder_path):\n",
    "    all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    df_list = []\n",
    "    for file in all_files:\n",
    "        print(f\"Loading: {file}\")\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Paths\n",
    "path_2017 = \"C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2017_dataset\"\n",
    "path_2018 = \"C:/Users/GPU RTX 5000/Desktop/Major Project Dataset/major/major_merge/cicids2018_dataset\"\n",
    "\n",
    "# Load data\n",
    "df_2017 = load_merge_files(path_2017)\n",
    "df_2018 = load_merge_files(path_2018)\n",
    "\n",
    "# Fuzzy match columns\n",
    "def fuzzy_align_columns(cols_src, cols_target, threshold=85):\n",
    "    rename_map = {}\n",
    "    for col in cols_src:\n",
    "        match, score, _ = process.extractOne(col, cols_target, scorer=fuzz.token_sort_ratio)\n",
    "        if score >= threshold:\n",
    "            rename_map[match] = col\n",
    "    return rename_map\n",
    "\n",
    "rename_dict = fuzzy_align_columns(df_2017.columns, df_2018.columns)\n",
    "df_2018_renamed = df_2018.rename(columns=rename_dict)\n",
    "\n",
    "# Keep common columns\n",
    "common_cols = df_2017.columns.intersection(df_2018_renamed.columns)\n",
    "df_2017 = df_2017[common_cols]\n",
    "df_2018_renamed = df_2018_renamed[common_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fce7b",
   "metadata": {},
   "source": [
    "## Save the Merged File Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7955317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged dataset saved as 'CICIDS2017_2018_Merged_Fuzzy.csv'\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_2017, df_2018_renamed], ignore_index=True).dropna()\n",
    "\n",
    "# Save the merged file\n",
    "df.to_csv(\"CICIDS2017_2018_Merged_Fuzzy.csv\", index=False)\n",
    "print(\"✅ Merged dataset saved as 'CICIDS2017_2018_Merged_Fuzzy.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c6cc6",
   "metadata": {},
   "source": [
    "## Cell 3: Clean Labels and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac17cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_label(label):\n",
    "    if not isinstance(label, str):\n",
    "        return 'Attack'\n",
    "    label = label.strip().lower()\n",
    "    label = unicodedata.normalize(\"NFKD\", label).encode(\"ascii\", \"ignore\").decode()\n",
    "    label = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", label)\n",
    "    return label.replace(\"__\", \"_\")\n",
    "\n",
    "df_2017['Label'] = df_2017['Label'].apply(clean_label)\n",
    "df_2018_renamed['Label'] = df_2018_renamed['Label'].apply(clean_label)\n",
    "\n",
    "# Binary label for stage 1\n",
    "df_2017['BinaryLabel'] = df_2017['Label'].apply(lambda x: 'Benign' if 'benign' in x else 'Malicious')\n",
    "df_2018_renamed['BinaryLabel'] = df_2018_renamed['Label'].apply(lambda x: 'Benign' if 'benign' in x else 'Malicious')\n",
    "\n",
    "df = pd.concat([df_2017, df_2018_renamed], ignore_index=True).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_2017, df_2018_renamed], ignore_index=True).dropna()\n",
    "\n",
    "# Save the merged file\n",
    "df.to_csv(\"CICIDS2017_2018_Merged_Fuzzy.csv\", index=False)\n",
    "print(\"✅ Merged dataset saved as 'CICIDS2017_2018_Merged_Fuzzy.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631a3d0",
   "metadata": {},
   "source": [
    "## Cell 4: Feature Selection (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a3223c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Random Forest for top features\u001b[39;00m\n\u001b[0;32m     10\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_binary_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Select top 40 features\u001b[39;00m\n\u001b[0;32m     14\u001b[0m feat_imp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(rf\u001b[38;5;241m.\u001b[39mfeature_importances_, index\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\GPU RTX 5000\\Desktop\\Major Project Dataset\\major\\major_merge\\myvenv\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GPU RTX 5000\\Desktop\\Major Project Dataset\\major\\major_merge\\myvenv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:375\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n\u001b[0;32m    374\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 375\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_missing_values_in_feature_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\GPU RTX 5000\\Desktop\\Major Project Dataset\\major\\major_merge\\myvenv\\lib\\site-packages\\sklearn\\tree\\_classes.py:222\u001b[0m, in \u001b[0;36mBaseDecisionTree._compute_missing_values_in_feature_mask\u001b[1;34m(self, X, estimator_name)\u001b[0m\n\u001b[0;32m    218\u001b[0m     overall_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(X)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(overall_sum):\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Raise a ValueError in case of the presence of an infinite element.\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     _assert_all_finite_element_wise(X, xp\u001b[38;5;241m=\u001b[39mnp, allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_kwargs)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# If the sum is not nan, then there are no missing values\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(overall_sum):\n",
      "File \u001b[1;32mc:\\Users\\GPU RTX 5000\\Desktop\\Major Project Dataset\\major\\major_merge\\myvenv\\lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['Label', 'BinaryLabel'])\n",
    "X = X.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "y_binary = df.loc[X.index, 'BinaryLabel']\n",
    "\n",
    "# Encode binary labels\n",
    "le_bin = LabelEncoder()\n",
    "y_binary_encoded = le_bin.fit_transform(y_binary)\n",
    "\n",
    "# Random Forest for top features\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y_binary_encoded)\n",
    "\n",
    "# Select top 40 features\n",
    "feat_imp = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "top_features = feat_imp.sort_values(ascending=False).head(40).index.tolist()\n",
    "X = X[top_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00481d",
   "metadata": {},
   "source": [
    "## 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ddd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full merged dataset with cleaned labels\n",
    "df.to_csv(\"CICIDS2017_2018_Merged_Cleaned.csv\", index=False)\n",
    "\n",
    "# Save the top 40 selected features with binary labels\n",
    "df_selected = X.copy()\n",
    "df_selected['BinaryLabel'] = y_binary\n",
    "df_selected.to_csv(\"CICIDS2017_2018_SelectedFeatures_Binary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bebbed",
   "metadata": {},
   "source": [
    "## file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5114a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save LabelEncoder\n",
    "joblib.dump(le_bin, 'binary_label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810faa0",
   "metadata": {},
   "source": [
    "## Cell 5: Stage 1 - Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f555501",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary_encoded, test_size=0.2, random_state=42, stratify=y_binary_encoded\n",
    ")\n",
    "\n",
    "# Binary Classifier (Random Forest)\n",
    "rf_stage1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_stage1.fit(X_train, y_train)\n",
    "binary_preds = rf_stage1.predict(X_test)\n",
    "\n",
    "print(\"=== Stage 1: Binary Classification ===\")\n",
    "print(confusion_matrix(y_test, binary_preds))\n",
    "print(classification_report(y_test, binary_preds, target_names=le_bin.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fd2d3",
   "metadata": {},
   "source": [
    "## Cell 6: Stage 2 - Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fdbe3",
   "metadata": {},
   "source": [
    "# Filter malicious only\n",
    "malicious_idx = np.where(binary_preds == 1)[0]\n",
    "X_malicious = X_test[malicious_idx]\n",
    "y_multiclass = df.loc[X.index, 'Label']\n",
    "y_multiclass_encoded = LabelEncoder().fit_transform(y_multiclass)\n",
    "y_mal_test = y_multiclass_encoded[y_test == 1]\n",
    "\n",
    "# Train multiclass model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train[y_train == 1], y_multiclass_encoded[y_train == 1])\n",
    "\n",
    "multi_preds = xgb_model.predict(X_malicious)\n",
    "\n",
    "print(\"=== Stage 2: Multiclass Classification ===\")\n",
    "print(confusion_matrix(y_mal_test, multi_preds))\n",
    "print(classification_report(y_mal_test, multi_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
